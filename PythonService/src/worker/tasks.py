import asyncio
import logging
import os
import json
from celery import Celery
from celery.signals import worker_ready
from sqlalchemy import select, update, func, desc
from sqlalchemy.dialects.postgresql import insert
from sqlalchemy.ext.asyncio import AsyncSession
from datasets import load_dataset # Cáº§n cÃ i thÆ° viá»‡n datasets

from src.core.cache import get_redis_client, close_redis_client
from src.core.models import TranslationLexicon
from src.core.session import AsyncSessionLocal

# LÆ°u Ã½: Báº¡n cÃ³ thá»ƒ bá» dÃ²ng import ingest_huggingface_loader cÅ© Ä‘i vÃ¬ ta sáº½ viáº¿t logic trá»±c tiáº¿p á»Ÿ dÆ°á»›i
# from src.worker.huggingface_loader import ingest_huggingface_data 

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

REDIS_HOST = os.getenv("REDIS_HOST", "localhost")
REDIS_PASSWORD = os.getenv("REDIS_PASSWORD", "redisPass123")
REDIS_URL = f"redis://:{REDIS_PASSWORD}@{REDIS_HOST}:6379/0"

celery_app = Celery("lingua_worker", broker=REDIS_URL, backend=REDIS_URL)

celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="Asia/Ho_Chi_Minh",
    enable_utc=True,
)

# --- Cáº¤U HÃŒNH DATASET & Tá»ª ÄIá»‚N THá»¦ CÃ”NG (Äá»‚ FIX Lá»–I Tá»ª ÄÆ N) ---
DATASET_SOURCES = [
    {
        "name": "Helsinki-NLP/opus-100",
        "config": "en-vi",
        "split": "train",
        "mapping": {"en": "en", "vi": "vi"},
        "limit": 150000 
    },
    {
        "name": "Helsinki-NLP/opus-100",
        "config": "en-zh",
        "split": "train",
        "mapping": {"en": "en", "zh": "zh"},
        "limit": 100000
    }
]

# ÄÃ¢y khÃ´ng pháº£i fake data, Ä‘Ã¢y lÃ  "Seed Data" Ä‘á»ƒ fix lá»—i Opus-100 thiáº¿u tá»« vá»±ng cÆ¡ báº£n
COMMON_BASIC_WORDS = [
    {"en": "hello", "vi": "xin chÃ o", "zh": "ä½ å¥½"},
    {"en": "hi", "vi": "chÃ o", "zh": "å—¨"},
    {"en": "love", "vi": "yÃªu", "zh": "çˆ±"},
    {"en": "you", "vi": "báº¡n", "zh": "ä½ "},
    {"en": "me", "vi": "tÃ´i", "zh": "æˆ‘"},
    {"en": "name", "vi": "tÃªn", "zh": "åå­—"},
    {"en": "what", "vi": "cÃ¡i gÃ¬", "zh": "ä»€ä¹ˆ"},
    {"en": "yes", "vi": "cÃ³", "zh": "æ˜¯"},
    {"en": "no", "vi": "khÃ´ng", "zh": "ä¸"},
    {"en": "thanks", "vi": "cáº£m Æ¡n", "zh": "è°¢è°¢"},
]

def normalize_text(text: str) -> str:
    if not text: return ""
    return text.strip().lower()

def get_redis_key(lang: str, text: str) -> str:
    return f"lex:{lang}:{normalize_text(text)}"

def run_async(coro):
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
    if loop.is_closed():
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
    return loop.run_until_complete(coro)

# --- TRIGGER ON STARTUP ---
@worker_ready.connect
def trigger_startup_tasks(sender, **kwargs):
    """Tá»± Ä‘á»™ng cháº¡y task ingest khi worker khá»Ÿi Ä‘á»™ng"""
    with sender.app.connection() as conn:
        sender.app.send_task("src.worker.tasks.ingest_huggingface_task", connection=conn)
        logger.info("ðŸš€ [Startup] Triggered ingest_huggingface_task automatically.")

# ==============================================================================
#  TASK 1: INGEST DATA (ÄÃƒ Cáº¬P NHáº¬T LOGIC FIX Lá»–I HELLO)
# ==============================================================================
@celery_app.task(name="src.worker.tasks.ingest_huggingface_task")
def ingest_huggingface_task():
    logger.info("ðŸš€ [Celery] Starting Hugging Face + Basic Dictionary Ingestion...")
    
    async def wrapper():
        redis = await get_redis_client()
        try:
            pipeline = redis.pipeline()
            
            # 1. Náº¡p tá»« Ä‘iá»ƒn thá»§ cÃ´ng trÆ°á»›c (Æ¯u tiÃªn cao Ä‘á»ƒ fix lá»—i tá»« Ä‘Æ¡n)
            logger.info("ðŸ› ï¸ Ingesting Basic Dictionary (Hello, Love, etc.)...")
            for entry in COMMON_BASIC_WORDS:
                if "en" in entry and "vi" in entry:
                    # En -> Vi
                    pipeline.hset(get_redis_key("en", entry["en"]), mapping={"vi": entry["vi"]})
                    # Vi -> En
                    pipeline.hset(get_redis_key("vi", entry["vi"]), mapping={"en": entry["en"]})
                if "en" in entry and "zh" in entry:
                    pipeline.hset(get_redis_key("en", entry["en"]), mapping={"zh-CN": entry["zh"]})
                    pipeline.hset(get_redis_key("zh-CN", entry["zh"]), mapping={"en": entry["en"]})
            await pipeline.execute()

            # 2. Náº¡p HuggingFace
            total_processed = 0
            for source in DATASET_SOURCES:
                logger.info(f"ðŸ“¥ Loading dataset: {source['name']} ({source['config']})....")
                try:
                    # Cháº¡y hÃ m synchronous load_dataset trong thread riÃªng Ä‘á»ƒ khÃ´ng cháº·n loop
                    dataset = await asyncio.to_thread(load_dataset, source['name'], source['config'], split=source['split'])
                    
                    if source.get("limit"):
                        limit = min(len(dataset), source["limit"])
                        dataset = dataset.select(range(limit))
                        
                    src_code = list(source["mapping"].keys())[0]
                    tgt_code = list(source["mapping"].keys())[1]
                    target_src_key = source["mapping"][src_code]
                    target_tgt_key = source["mapping"][tgt_code]

                    count = 0
                    batch_pipe = redis.pipeline()
                    
                    for row in dataset:
                        translation = row.get("translation", {})
                        text_src = translation.get(src_code, "")
                        text_tgt = translation.get(tgt_code, "")

                        if not text_src or not text_tgt: continue
                        
                        # Normalize & Save
                        key_src = get_redis_key(target_src_key, text_src.strip())
                        batch_pipe.hset(key_src, mapping={target_tgt_key: text_tgt.strip()})
                        batch_pipe.expire(key_src, 60 * 60 * 24 * 60)

                        key_tgt = get_redis_key(target_tgt_key, text_tgt.strip())
                        batch_pipe.hset(key_tgt, mapping={target_src_key: text_src.strip()})
                        batch_pipe.expire(key_tgt, 60 * 60 * 24 * 60)

                        count += 1
                        if count % 2000 == 0:
                            await batch_pipe.execute()
                    
                    await batch_pipe.execute()
                    total_processed += count
                    logger.info(f"âœ¨ Processed {count} pairs from {source['config']}")
                    
                except Exception as e:
                    logger.error(f"âŒ Failed to load {source['name']}: {e}")
                    continue

        except Exception as e:
            logger.error(f"Error inside wrapper: {e}")
            raise e
        finally:
            await close_redis_client()

    try:
        run_async(wrapper())
        logger.info("âœ… [Celery] Ingestion Completed Successfully.")
        return "Success"
    except Exception as e:
        logger.error(f"âŒ [Celery] Task Failed: {e}")
        return f"Failed: {str(e)}"

# ==============================================================================
#  TASK 2: RUNTIME SYNC (GIá»® NGUYÃŠN CODE Cá»¦A Báº N)
# ==============================================================================
@celery_app.task(name="src.worker.tasks.sync_translation_to_db_task")
def sync_translation_to_db_task(original_text: str, src_lang: str, translations: dict):
    async def _logic():
        async with AsyncSessionLocal() as session:
            try:
                # Logic Upsert cá»§a báº¡n giá»¯ nguyÃªn
                stmt = insert(TranslationLexicon).values(
                    original_text=original_text,
                    original_lang=src_lang,
                    translations=translations,
                    usage_count=1
                )
                stmt = stmt.on_conflict_do_update(
                    index_elements=['id'], # Äáº£m báº£o DB cÃ³ unique index hoáº·c dÃ¹ng constraint name
                    set_={
                        "translations": translations,
                        "usage_count": TranslationLexicon.usage_count + 1,
                        "last_used_at": func.now()
                    }
                )
                
                # Fallback check náº¿u on_conflict fail do dialect
                existing = await session.execute(
                    select(TranslationLexicon).where(
                        TranslationLexicon.original_text == original_text,
                        TranslationLexicon.original_lang == src_lang
                    )
                )
                lexicon_entry = existing.scalar_one_or_none()

                if lexicon_entry:
                    current_data = dict(lexicon_entry.translations) if lexicon_entry.translations else {}
                    current_data.update(translations)
                    lexicon_entry.translations = current_data
                    lexicon_entry.usage_count += 1
                    lexicon_entry.last_used_at = func.now()
                else:
                    new_entry = TranslationLexicon(
                        original_text=original_text,
                        original_lang=src_lang,
                        translations=translations,
                        usage_count=1
                    )
                    session.add(new_entry)
                
                await session.commit()
                logger.info(f"ðŸ’¾ Runtime Sync to DB: {original_text}")
            except Exception as e:
                logger.error(f"DB Sync Failed: {e}")
                await session.rollback()

    run_async(_logic())

# ==============================================================================
#  TASK 3: BACKUP REDIS TO DB (GIá»® NGUYÃŠN CODE Cá»¦A Báº N)
# ==============================================================================
@celery_app.task(name="src.worker.tasks.backup_redis_to_db_task")
def backup_redis_to_db_task():
    logger.info("ðŸ’¾ [Backup] Starting Redis -> DB Backup...")
    
    async def _logic():
        redis = await get_redis_client()
        async with AsyncSessionLocal() as session:
            cursor = b'0'
            count = 0
            BATCH_SIZE = 500

            while cursor:
                cursor, keys = await redis.scan(cursor, match="lex:*", count=1000)
                for key in keys:
                    key_str = key.decode("utf-8")
                    parts = key_str.split(":")
                    if len(parts) < 3: continue
                    
                    lang = parts[1]
                    # Fix: Náº¿u text cÃ³ chá»©a dáº¥u : thÃ¬ pháº£i join láº¡i pháº§n cÃ²n láº¡i
                    text = ":".join(parts[2:]) 

                    data = await redis.hgetall(key)
                    if not data: continue

                    translations = {}
                    usage = 0
                    
                    for k, v in data.items():
                        k_str = k.decode("utf-8")
                        v_str = v.decode("utf-8")
                        if k_str == "usage":
                            try:
                                usage = int(v_str)
                            except:
                                usage = 0
                        else:
                            translations[k_str] = v_str
                    
                    if not translations: continue

                    # Update DB
                    stmt = select(TranslationLexicon).where(
                        TranslationLexicon.original_text == text,
                        TranslationLexicon.original_lang == lang
                    )
                    res = await session.execute(stmt)
                    entry = res.scalar_one_or_none()

                    if entry:
                        current_trans = dict(entry.translations) if entry.translations else {}
                        current_trans.update(translations)
                        entry.translations = current_trans
                        entry.usage_count = max(entry.usage_count, usage)
                        entry.last_used_at = func.now()
                    else:
                        new_entry = TranslationLexicon(
                            original_text=text,
                            original_lang=lang,
                            translations=translations,
                            usage_count=usage
                        )
                        session.add(new_entry)
                    
                    count += 1
                    if count % BATCH_SIZE == 0:
                        await session.commit()
                        logger.info(f"   Saved {count} items...")

            await session.commit()
            logger.info(f"âœ… Backup complete. Total items processed: {count}")

    run_async(_logic())

# ==============================================================================
#  TASK 4: RESTORE DB TO REDIS (GIá»® NGUYÃŠN CODE Cá»¦A Báº N)
# ==============================================================================
@celery_app.task(name="src.worker.tasks.restore_db_to_redis_task")
def restore_db_to_redis_task():
    logger.info("ðŸ”¥ [Restore] Starting DB -> Redis Warmup...")
    
    async def _logic():
        redis = await get_redis_client()
        async with AsyncSessionLocal() as session:
            stmt = select(TranslationLexicon).order_by(
                desc(TranslationLexicon.usage_count)
            ).limit(50000)
            
            result = await session.execute(stmt)
            entries = result.scalars().all()
            
            pipeline = redis.pipeline()
            for entry in entries:
                key = f"lex:{entry.original_lang}:{normalize_text(entry.original_text)}"
                mapping_data = dict(entry.translations) if entry.translations else {}
                mapping_data["usage"] = str(entry.usage_count)
                
                pipeline.hset(key, mapping=mapping_data)
                pipeline.expire(key, 60*60*24*30)
            
            await pipeline.execute()
            logger.info(f"âœ… Restored {len(entries)} 'hot' words to Redis.")
 
    run_async(_logic())