# # # FILE: src/scripts/seed_lexicon.py

# # import csv
# # import logging
# # import asyncio
# # import os
# # from sqlalchemy import select
# # from sqlalchemy.dialects.postgresql import insert
# # from src.core.session import AsyncSessionLocal
# # from src.core.models import TranslationLexicon
# # from src.worker.tasks import warm_up_redis_task
# # from datetime import datetime, timezone, timedelta
# # from sqlalchemy.sql import func

# # logging.basicConfig(level=logging.INFO)
# # logger = logging.getLogger(__name__)

# # DATA_FILE_PATH = "/app/PythonService/src/data/dictionary.csv"

# # VN_TZ = timezone(timedelta(hours=7))

# # STATIC_TIMESTAMP_NAIVE = datetime.strptime("2025-01-01 00:00:00", "%Y-%m-%d %H:%M:%S")
# # STATIC_TIMESTAMP = STATIC_TIMESTAMP_NAIVE.replace(tzinfo=VN_TZ)

# # async def seed_data():
# #     if not os.path.exists(DATA_FILE_PATH):
# #         logger.error(f"File not found: {DATA_FILE_PATH}")
# #         return

# #     async with AsyncSessionLocal() as session:
# #         batch_size = 5000
# #         batch_data = []
# #         total_inserted = 0

# #         with open(DATA_FILE_PATH, 'r', encoding='utf-8', errors='ignore') as f:
# #             for line in f:
# #                 parts = line.strip().split('\t')
# #                 if len(parts) < 2:
# #                     parts = line.strip().split(',')
                
# #                 if len(parts) >= 2:
                    
# #                     # C·ªòT G·ªêC ƒëang ch·ª©a c·∫£ 'vi' v√† 'en' (VD: 'a la h√°n  arhant')
# #                     # T√°ch t·ª´ ƒë·∫ßu ti√™n (ng√¥n ng·ªØ g·ªëc) v√† b·∫£n d·ªãch c√≤n l·∫°i
                    
# #                     # L·∫•y to√†n b·ªô n·ªôi dung c·ªôt 0
# #                     full_original_col = parts[0].strip().lower()
                    
# #                     # T√°ch theo kho·∫£ng tr·∫Øng ƒë·∫ßu ti√™n
# #                     split_on_space = full_original_col.split(' ', 1)

# #                     if len(split_on_space) > 1:
# #                         # N·∫øu t√°ch th√†nh c√¥ng (c√≥ kho·∫£ng tr·∫Øng)
# #                         original = split_on_space[0].strip()
# #                         # Ph·∫ßn c√≤n l·∫°i c·ªßa c·ªôt 0 ƒë∆∞·ª£c th√™m v√†o b·∫£n d·ªãch c≈© (c·ªôt 1)
# #                         translated_from_col_0 = split_on_space[1].strip()
# #                         translated_from_col_1 = parts[1].strip() if len(parts) > 1 else ""
                        
# #                         # G·ªôp t·∫•t c·∫£ c√°c ph·∫ßn b·∫£n d·ªãch l·∫°i
# #                         translated_list = [t for t in [translated_from_col_0, translated_from_col_1] if t]
# #                         translated_text = ', '.join(translated_list)
# #                     else:
# #                         # N·∫øu c·ªôt 0 ch·ªâ ch·ª©a t·ª´ g·ªëc (kh√¥ng c√≥ kho·∫£ng tr·∫Øng)
# #                         original = full_original_col
# #                         translated_text = parts[1].strip() if len(parts) > 1 else ""

# #                     if len(original) > 1 and len(translated_text) > 1:
# #                         batch_data.append({
# #                             "original_text": original,
# #                             "original_lang": "vi",
# #                             "translations": {"en": translated_text},
# #                             "usage_count": 100, 
# #                             "last_used_at": STATIC_TIMESTAMP,
# #                         })

# #                 if len(batch_data) >= batch_size:
# #                     stmt = insert(TranslationLexicon).values(batch_data)
# #                     stmt = stmt.on_conflict_do_nothing(
# #                         index_elements=['original_text', 'original_lang']
# #                     )
# #                     await session.execute(stmt)
# #                     await session.commit()
# #                     total_inserted += len(batch_data)
# #                     logger.info(f"Inserted batch: {total_inserted} words")
# #                     batch_data = []

# #             if batch_data:
# #                 stmt = insert(TranslationLexicon).values(batch_data)
# #                 stmt = stmt.on_conflict_do_nothing(
# #                     index_elements=['original_text', 'original_lang']
# #                 )
# #                 await session.execute(stmt)
# #                 await session.commit()
# #                 total_inserted += len(batch_data)
            
# #             logger.info(f"Seeding complete. Total: {total_inserted} words.")
# #             warm_up_redis_task.delay()

# # if __name__ == "__main__":
# #     asyncio.run(seed_data())
# ## üõ†Ô∏è src/scripts/seed_lexicon.py (C·∫≠p nh·∫≠t)

# import logging
# import asyncio
# import os
# from sqlalchemy import select, func # Th√™m select v√† func
# from sqlalchemy.dialects.postgresql import insert
# from src.core.session import AsyncSessionLocal
# from src.core.models import TranslationLexicon
# from src.worker.tasks import warm_up_redis_task
# from datetime import datetime, timezone, timedelta

# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger(__name__)

# # CH·ªàNH S·ª¨A T√äN FILE N·∫æU C·∫¶N: ƒê·ªïi t·ª´ dictionary.csv sang dictionary.txt
# DATA_FILE_PATH = "/app/PythonService/src/data/dictionary.txt"

# VN_TZ = timezone(timedelta(hours=7))

# STATIC_TIMESTAMP_NAIVE = datetime.strptime("2025-01-01 00:00:00", "%Y-%m-%d %H:%M:%S")
# STATIC_TIMESTAMP = STATIC_TIMESTAMP_NAIVE.replace(tzinfo=VN_TZ)

# async def seed_data():
#     if not os.path.exists(DATA_FILE_PATH):
#         logger.error(f"File not found: {DATA_FILE_PATH}")
#         return

#     async with AsyncSessionLocal() as session:
#         # 1. KI·ªÇM TRA S·ªê L∆Ø·ª¢NG B·∫¢N GHI HI·ªÜN C√ì 
#         try:
#             # ƒê·∫øm s·ªë l∆∞·ª£ng b·∫£n ghi trong b·∫£ng TranslationLexicon
#             count_stmt = select(func.count()).select_from(TranslationLexicon)
#             result = await session.execute(count_stmt)
#             count = result.scalar_one()

#             if count > 0:
#                 logger.info(f"B·∫£ng TranslationLexicon ƒë√£ c√≥ {count} b·∫£n ghi. B·ªè qua b∆∞·ªõc Seeding.")
#                 # V·∫´n g·ªçi warm_up_redis_task ƒë·ªÉ ƒë·∫£m b·∫£o Redis ƒë∆∞·ª£c l√†m n√≥ng n·∫øu c·∫ßn
#                 warm_up_redis_task.delay()
#                 return # D·ª´ng h√†m n·∫øu b·∫£ng kh√¥ng tr·ªëng
            
#             logger.info("B·∫£ng TranslationLexicon tr·ªëng. B·∫Øt ƒë·∫ßu Seeding d·ªØ li·ªáu...")

#         except Exception as e:
#             logger.error(f"L·ªói khi ki·ªÉm tra b·∫£ng: {e}")
#             return # D·ª´ng n·∫øu c√≥ l·ªói khi ki·ªÉm tra

#         # 2. B·∫ÆT ƒê·∫¶U SEEDING N·∫æU B·∫¢NG TR·ªêNG
#         batch_size = 5000
#         batch_data = []
#         total_inserted = 0

#         current_original_text = None
#         current_translations = []

#         with open(DATA_FILE_PATH, 'r', encoding='utf-8', errors='ignore') as f:
#             for line in f:
#                 line = line.strip()
#                 if not line:
#                     continue

#                 if line.startswith('@'):
#                     # B·∫Øt ƒë·∫ßu m·ªôt entry m·ªõi, l∆∞u entry c≈© n·∫øu c√≥
#                     if current_original_text and current_translations:
#                         translated_text = ', '.join(current_translations)
#                         if len(current_original_text) > 1 and len(translated_text) > 1:
#                             batch_data.append({
#                                 "original_text": current_original_text,
#                                 "original_lang": "vi",
#                                 "translations": {"en": translated_text},
#                                 "usage_count": 100, 
#                                 "last_used_at": STATIC_TIMESTAMP,
#                             })
#                             total_inserted += 1
                        
#                         # Ki·ªÉm tra v√† flush batch n·∫øu c·∫ßn
#                         if len(batch_data) >= batch_size:
#                             stmt = insert(TranslationLexicon).values(batch_data)
#                             stmt = stmt.on_conflict_do_nothing(
#                                 index_elements=['original_text', 'original_lang']
#                             )
#                             await session.execute(stmt)
#                             await session.commit()
#                             logger.info(f"Inserted batch: {total_inserted} words")
#                             batch_data = []

#                     # Reset v√† kh·ªüi t·∫°o entry m·ªõi
#                     current_original_text = line[1:].strip().lower()
#                     current_translations = []

#                 elif current_original_text and line.startswith('-'):
#                     # Th√™m b·∫£n d·ªãch
#                     translation_line = line[1:].strip()
#                     translation_parts = [t.strip() for t in translation_line.split(',')]
#                     current_translations.extend(translation_parts)

#                 elif current_original_text and line.startswith('='):
#                     # B·ªè qua d√≤ng v√≠ d·ª•
#                     pass

#                 elif current_original_text and line.startswith('*'):
#                     # B·ªè qua d√≤ng metadata (part-of-speech)
#                     pass

#             # X·ª≠ l√Ω entry cu·ªëi c√πng sau khi h·∫øt file
#             if current_original_text and current_translations:
#                 translated_text = ', '.join(current_translations)
#                 if len(current_original_text) > 1 and len(translated_text) > 1:
#                     batch_data.append({
#                         "original_text": current_original_text,
#                         "original_lang": "vi",
#                         "translations": {"en": translated_text},
#                         "usage_count": 100, 
#                         "last_used_at": STATIC_TIMESTAMP,
#                     })
#                     total_inserted += 1

#             # Flush batch cu·ªëi c√πng
#             if batch_data:
#                 stmt = insert(TranslationLexicon).values(batch_data)
#                 stmt = stmt.on_conflict_do_nothing(
#                     index_elements=['original_text', 'original_lang']
#                 )
#                 await session.execute(stmt)
#                 await session.commit()
            
#             logger.info(f"Seeding complete. Total: {total_inserted} words.")
#             warm_up_redis_task.delay()

# if __name__ == "__main__":
#     asyncio.run(seed_data())